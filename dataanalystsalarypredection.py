# -*- coding: utf-8 -*-
"""DataAnalystSalaryPredection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qXVJR3kELilTDmvKoqFw9DJAOZEqEGwx
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/Data Mining Project/DataAnalyst.csv')
df.head()

df.info()

df = df.replace([-1, '-1'], np.nan)

df.info()

df = df[df['Salary Estimate'].notna()]

df[df['Headquarters'].isna()].iloc[:5,5:]

# Step 1: Filter out rows with null values in 'Sector' column
df_sector_notnull = df[df['Sector'].notnull()]

# Step 2: Filter out rows with null values in 'Rating' column
df_rating_notnull = df_sector_notnull[df_sector_notnull['Rating'].notnull()]

# Step 3: Filter out rows with null values in 'Headquarters' column
df_headquarters_notnull = df_rating_notnull[df_rating_notnull['Headquarters'].notnull()]

# Step 4: Reset the index and drop the old index column
df = df_headquarters_notnull.reset_index(drop=True)

# Step 5: Check the cleaned DataFrame's structure
df.info()

df.isnull().sum()

df=df.drop(['Unnamed: 0'], axis=1)

#Salary Estimate cleaning
salary = df['Salary Estimate'].str.split('-', expand=True)
salary.iloc[:, 0] = salary.iloc[:, 0].str.extract('(\d+)', expand=False)
salary.iloc[:, 1] = salary.iloc[:, 1].str.extract('(\d+)', expand=False)

salary = salary.astype('int32')

df['Higher estimate'] = salary.iloc[:, 1]
df['Lower estimate'] = salary.iloc[:, 0]

df['Lower estimate'].describe()
df['Lower estimate'].hist(bins=100)

df['Higher estimate'].hist(bins=100)

df["Salary_average"]=df['Lower estimate']+df['Higher estimate']/2
df["Salary_average"]=df["Salary_average"].astype('int32')

df['Salary_average'].hist(bins=100)

df.sample(5)

"""**Rating**"""

df['Rating'].sample(10)

df['Rating'].isna().sum()

df['Rating'].hist(bins=50)

"""**Founded**"""

df['Founded'].isna().sum()

# Handling -1 values in Founded column
df['Founded'] = df['Founded'].apply(lambda x: np.NaN if x==-1 else x)
df['Founded'] = df['Founded'].fillna(int(df['Founded'].median()))
df['Founded'] = df['Founded'].astype('int')

df['Founded'].isna().sum()

df['Founded'].hist(bins=100)

"""**Job title**"""

df['Job Title']= df['Job Title'].str.replace ('Sr. Data Analyst','Senior Data Analyst', regex= True)
df['Job Title']= df['Job Title'].str.replace ('Sr Data Analyst','Senior Data Analyst')
df['Job Title']= df['Job Title'].str.replace ('Data Analyst Senior','Senior Data Analyst')
df['Job Title']= df['Job Title'].str.replace ('Jr. Data Analyst','Junior Data Analyst', regex= True)
df['Job Title']= df['Job Title'].str.replace ('Jr Data Analyst','Junior Data Analyst')
df['Job Title']= df['Job Title'].str.replace ('Data Analyst Junior','Junior Data Analyst')

df['Job Title'].value_counts()[:20]

df['data_analyst'] = df['Job Title'].str.contains('data analyst', na=False, case=False).astype(int)
df['senior_data_analyst'] = df['Job Title'].str.contains('senior data analyst', na=False, case=False).astype(int)
df['junior_data_analyst'] = df['Job Title'].str.contains('junior data analyst', na=False, case=False).astype(int)
df['business_data_analyst'] = df['Job Title'].str.contains('business data analyst', na=False, case=False).astype(int)

"""**Company Name**"""

df['Company Name'].sample(10)

df[['Company Name', 'Rating']].sample(10)

#At the end of the company name rating is getting associated with it so we can remove rating from comany name column
df['Company Name']= df['Company Name'].str.split('\n').str[0]
df['Company Name'].head()

"""**Location**"""

df['Location'].isnull().sum()

# Creating a column 'job_state'
df['job_location'] = df['Location'].apply(lambda x: x if ',' not in x else x.split(',')[1].strip())
print('Total number of unique locations: {}'.format(len(df['job_location'].unique())))

df['job_location'].value_counts()[:20]

# Trimming the 'job_location' column

# Taking top 9 States and replacing others by 'Others'
job_location_list = list(df['job_location'].value_counts()[:9].index)

def job_location_simplifier(text):
  if text not in job_location_list:
    return 'Others'
  else:
    return text

df['job_location'] = df['job_location'].apply(job_location_simplifier)
df['job_location'].value_counts()[:9]

df['state_CA'] = df['job_location'].str.contains('CA', na=False, case=False).astype(int)
df['state_TX'] = df['job_location'].str.contains('TX', na=False, case=False).astype(int)
df['state_NY'] = df['job_location'].str.contains('NY', na=False, case=False).astype(int)
df['state_IL'] = df['job_location'].str.contains('IL', na=False, case=False).astype(int)
df['state_PA'] = df['job_location'].str.contains('PA', na=False, case=False).astype(int)

"""**Headquarters**"""

# # Creating column of 'job_in_headquarters'
# df['job_in_headquarters'] = df.apply(lambda x: 1 if x['Location'] == x['Headquarters'] else 0, axis=1)

"""**Size**"""

df['Size'].sample(10)

df['Size'].isnull().sum()

"""**Type of ownership**"""

df['Type of ownership'].isna().sum()

df['Type of ownership'].value_counts()

def ownership_simplifier(text):
    if 'private' in text.lower():
        return 'Private'
    elif 'public' in text.lower():
        return 'Public'
    elif ('-1' in text.lower()) or ('unknown' in text.lower()) or ('school / school district' in text.lower()) or ('private practice / firm' in text.lower()) or ('contract' in text.lower()) :
        return 'Other Organization'
    else:
        return text

df['Type of ownership'] = df['Type of ownership'].apply(ownership_simplifier)

df['Type of ownership'].value_counts()[:20]

"""**Industry**"""

df['Industry'].isnull().sum()

df['Industry'].value_counts()[:20]

df['Industry'] = df['Industry'].apply(lambda x: 'Others' if x=='-1' else x)

df['Industry'].value_counts()[:20]

"""**Sector**"""

df['Sector'].isna().sum()

df['Sector']

# Exclude the -1 values from the DataFrame before getting the top 9 sectors by frequency
sector_counts = df[df['Sector'] != '-1']['Sector'].value_counts()
sector_list = list(sector_counts[:9].index)
print(sector_list)
def sector_simplifier(text):
    if text == '-1':
        return 'Others'
    elif text not in sector_list:
        return 'Others'
    else:
        return text

df['Sector'] = df['Sector'].apply(sector_simplifier)
df['Sector'].value_counts()[:9]

"""**Job Description**"""

df['skill_python'] = df['Job Description'].str.contains('python', na=False, case=False).astype(int)
df['skill_SQL'] = df['Job Description'].str.contains('SQL', na=False, case=False).astype(int)
df['skill_excell'] = df['Job Description'].str.contains('Excel', na=False, case=False).astype(int)
df['skill_tableau'] = df['Job Description'].str.contains('Tableau', na=False, case=False).astype(int)
# df['skill_PowerBI'] = df['Job Description'].str.contains('PowerBI', na=False, case=False).astype(int)
# df['skill_R_programming'] = df['Job Description'].str.contains('R', na=False, case=False).astype(int)
# df['skill_java'] = df['Job Description'].str.contains('java', na=False, case=False).astype(int)

"""**Revenue**"""

# Cleaning 'Revenue' column
def revenue_simplifier(text):
    if '-1' in text.lower():
        return 'Unknown / Non-Applicable'
    else:
        return text
df['Revenue'] = df['Revenue'].apply(revenue_simplifier)
df['Revenue'].value_counts()[:9]

"""**Competitors**"""

df['Competitors']

#df['Competitors'] = df['Competitors'].apply(lambda x: len(x.split(',')) if x != 'nan' else 0)

df.info()



"""# **Visualizations**"""

import matplotlib.pyplot as plt

# define dimension_words list
dimension_words = ['senior', 'business', 'analytics', 'junior', 'quality',
                   'lead', 'management', 'reporting', 'healthcare', 'financial',
                   'governance', 'operations', 'security', 'warehouse']

# get frequency counts of dimension_words in Job Title column
freq = df['Job Title'].str.lower().str.split().apply(pd.Series).stack().value_counts()

# filter for dimension_words
dimension_word_freq = freq[freq.index.isin(dimension_words)].reset_index()

# plot bar chart
plt.bar(dimension_word_freq['index'], dimension_word_freq[0])
plt.xticks(rotation=45)
plt.xlabel('Dimension Words')
plt.ylabel('Frequency')
plt.title('Frequency of Dimension Words in Job Titles')
plt.show()

plt.figure(figsize=(10, 5))  #  size of the plot

sns.histplot(data=df, x='Rating', binwidth=0.2, color='blue')

plt.title('Company Rating Distribution')
plt.xlabel('Company rating')

plt.show()

#Graph to show salary distributions across different states
df[['Higher estimate', 'Lower estimate', 'job_location']].groupby('job_location').\
    mean().reset_index().plot.bar(x='job_location', stacked=True)

most_posts_title = df['Job Title'].value_counts()[:20].sort_values()

fig, ax = plt.subplots(figsize=(10,5))

bars = ax.barh(most_posts_title.index, most_posts_title.values)

ax.bar_label(bars, fmt='% d')

ax.set_xlim(right=700)

ax.set_title('Number of Job Posts per Job Title')
ax.set_xlabel('# of job posts')
plt.show()

plt.figure(figsize=(10,5))
g = df["Company Name"].value_counts().nlargest(15).plot(kind='barh')
p = plt.title('Top 15 Company Names')
p = plt.xlabel('Count')

plt.figure(figsize=(10,5))
g = sns.countplot(x='Size', data=df, order = df['Size'].value_counts().index)
p = plt.title('Count plot for Company Size')
p = plt.xlabel('Company Size')
p = plt.ylabel('Count')
p = g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')

fig, ax = plt.subplots(figsize=(10,5))

ax = sns.countplot(data=df, y='job_location', order=df['job_location'].value_counts().index, color='orange')

ax.set_title('Number of Data Analyst Job Posts per State')
ax.set_xlabel('# Job post')
ax.set_ylabel('')

plt.show()

plt.figure(figsize=(10,5))
g = sns.countplot(x='Sector', data=df, order = df['Sector'].value_counts().index)
p = plt.title('Final countplot for Sector')
p = plt.xlabel('Sector')
p = plt.ylabel('Count')
p = g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')

plt.figure(figsize=(10,5))
g = sns.countplot(x='Revenue', data=df, order = df['Revenue'].value_counts().index)
p = plt.title('Count plot for Revenue')
p = plt.xlabel('Revenue')
p = plt.ylabel('Count')
p = g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')

fig, (plt1) = plt.subplots(ncols=1, figsize=(10,5))
plt1.scatter(df['Rating'], df['Salary_average'])
plt1.set_xlabel('Company Rating')
plt1.set_ylabel('Average Salary ($)')
plt1.set_title('Company Rating vs Average Salary')

plt.show()

"""# **Model Training**"""

df.sample(2)

df.columns.tolist()

df.drop(labels=['Job Title', 'Salary Estimate', 'Job Description', 'Company Name', 'Location', 'Headquarters', 'Industry', 'job_location', 'Competitors', 'Higher estimate', 'Lower estimate'], axis=1, inplace=True)
df.head()

# Renaming columns
df.rename(columns={'Rating':'company_rating', 'Size':'company_size', 'Founded':'company_founded', 'Type of ownership':'type_of_ownership',
                   'Sector':'sector', 'Revenue':'revenue'}, inplace=True)
# Mapping ranks to 'company_size' column
size_map = {'Unknown': 0, '1 to 50 employees': 1, '51 to 200 employees': 2, '201 to 500 employees': 3,
            '501 to 1000 employees': 4, '1001 to 5000 employees': 5, '5001 to 10000 employees': 6, '10000+ employees': 7}

df['company_size'] = df['company_size'].map(size_map)

# Mapping ranks to 'revenue	' column
revenue_map = {'Unknown / Non-Applicable': 0, 'Less than $1 million (USD)': 1, '$1 to $5 million (USD)': 2, '$5 to $10 million (USD)': 3,
            '$10 to $25 million (USD)': 4, '$25 to $50 million (USD)': 5, '$50 to $100 million (USD)': 6, '$100 to $500 million (USD)': 7,
            '$500 million to $1 billion (USD)': 8, '$1 to $2 billion (USD)': 9, '$2 to $5 billion (USD)':10, '$5 to $10 billion (USD)':11,
            '$10+ billion (USD)':12}

df['revenue'] = df['revenue'].map(revenue_map)

# # Mapping ranks to 'job_seniority	' column
# job_seniority_map = {'other': 0, 'jr': 1, 'sr': 2}

# df['job_level'] = df['job_level'].map(job_seniority_map)

# Removing 'type_of_ownership' column using get_dummies()
print('Before: {}'.format(df.shape))
df = pd.get_dummies(columns=['type_of_ownership'], data=df, prefix='ownership')
print('After: {}'.format(df.shape))

# Removing 'sector' column using get_dummies()
print('Before: {}'.format(df.shape))
df = pd.get_dummies(columns=['sector'], data=df)
print('After: {}'.format(df.shape))

# # Removing 'job_title' column using get_dummies()
# print('Before: {}'.format(df.shape))
# df = pd.get_dummies(columns=['jobs'], data=df)
# print('After: {}'.format(df.shape))

df.sample(5)

# Removing 'others' column to reduce dimentionality and avoid dummy variable trap
df.drop(labels=['sector_Others', 'ownership_Other Organization','Easy Apply'], axis=1, inplace=True)

X = df[[ 'company_rating', 'company_size', 'company_founded', 'revenue',
        'data_analyst', 'senior_data_analyst',
        'junior_data_analyst', 'business_data_analyst', 'state_CA', 'state_TX',
        'state_NY', 'state_IL', 'state_PA', 'skill_python', 'skill_SQL',
        'skill_excell', 'skill_tableau', 'ownership_College / University',
       'ownership_Franchise', 'ownership_Government', 'ownership_Hospital',
       'ownership_Nonprofit Organization', 'ownership_Private', 'ownership_Public',
       'ownership_Self-employed', 'ownership_Subsidiary or Business Segment',
       'sector_Accounting & Legal', 'sector_Business Services', 'sector_Education',
       'sector_Finance', 'sector_Health Care', 'sector_Information Technology',
       'sector_Insurance', 'sector_Manufacturing', 'sector_Media']].reset_index(drop=True)

y = df['Salary_average']

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# select only the features
X = df[[ 'company_rating', 'company_size', 'company_founded', 'revenue',
        'data_analyst', 'senior_data_analyst',
        'junior_data_analyst', 'business_data_analyst', 'state_CA', 'state_TX',
        'state_NY', 'state_IL', 'state_PA', 'skill_python', 'skill_SQL',
        'skill_excell', 'skill_tableau', 'ownership_College / University',
       'ownership_Franchise', 'ownership_Government', 'ownership_Hospital',
       'ownership_Nonprofit Organization', 'ownership_Private', 'ownership_Public',
       'ownership_Self-employed', 'ownership_Subsidiary or Business Segment',
       'sector_Accounting & Legal', 'sector_Business Services', 'sector_Education',
       'sector_Finance', 'sector_Health Care', 'sector_Information Technology',
       'sector_Insurance', 'sector_Manufacturing', 'sector_Media']]

# scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# scale the target variable
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(df[['Salary_average']])

# perform PCA with n_components = number of features
pca = PCA(n_components=len(X.columns))
principal_components = pca.fit_transform(X_scaled)

# convert principal components to a Pandas DataFrame
principal_components = pd.DataFrame(principal_components, columns=[f'PC{i}' for i in range(1, len(X.columns)+1)])

# print the explained variance ratio for each principal component
print(pca.explained_variance_ratio_)

# get the explained variance ratio for each principal component
explained_variance_ratio = pca.explained_variance_ratio_

# print the explained variance ratio for each principal component
for i, ratio in enumerate(explained_variance_ratio):
    print(f"PC{i+1}: {ratio:.2f}")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(principal_components.iloc[:, :6], y, test_size=0.3, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor

# Create a pipeline for linear regression
linear_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('linear', LinearRegression())
])

# Create a pipeline for KNN
knn_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('knn', KNeighborsRegressor())
])

# Create a pipeline for Random Forest
rf_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('rf', RandomForestRegressor())
])

# Create a pipeline for Decision Tree
dt_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('dt', DecisionTreeRegressor())
])

# Create a pipeline for SVM
svm_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('svm', SVR())
])

# Create a pipeline for Boosted Tree
bt_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('xgb', XGBRegressor())
])

# Create a pipeline for MLP
mlp=MLPRegressor(learning_rate_init=0.1,hidden_layer_sizes=(100,100,100,),verbose=True)
mlp_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('mlp', mlp)
])

# Train the linear regression model
linear_pipe.fit(X_train, y_train)

# Train the KNN model
knn_pipe.fit(X_train, y_train)

# Train the random forest model
rf_pipe.fit(X_train, y_train)

# Train the decision tree model
dt_pipe.fit(X_train, y_train)

# Train the SVM model
svm_pipe.fit(X_train, y_train)

# Train the boosted tree model
bt_pipe.fit(X_train, y_train)

# Train the MLP model
mlp_pipe.fit(X_train,y_train)

from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics import r2_score

models = [linear_pipe, knn_pipe, rf_pipe, dt_pipe, svm_pipe, bt_pipe, mlp_pipe]

# Define a list of model names
model_names = ['Linear regression', 'KNN', 'Random forest', 'Decision tree', 'SVM', 'Boosted tree', 'MLP']

for i, model in enumerate(models):
    y_train_pred = model.predict(X_train)
    print("Model:", model_names[i])
    print("R2 score:", r2_score(y_train, y_train_pred))
    print("Mean squared error:", mean_squared_error(y_train, y_train_pred))
    print("Mean absolute error:", mean_absolute_error(y_train, y_train_pred))

from sklearn.model_selection import GridSearchCV

# define the pipeline
rf_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('rf', RandomForestRegressor())
])

# define the hyperparameters to tune
params_rf = {
    'rf__n_estimators': [50, 100, 200],
    'rf__max_depth': [5, 10, 15],
    'rf__min_samples_split': [2, 5, 10],
    'rf__min_samples_leaf': [1, 2, 4],
}

# define the grid search object
grid_rf = GridSearchCV(rf_pipe, params_rf, cv=5, n_jobs=-1, verbose=1)

# fit the grid search object to the data
grid_rf.fit(X_train, y_train)

# print the best parameters and score
print("Best parameters: ", grid_rf.best_params_)
print("Best score: ", grid_rf.best_score_)

# get the best model
best_rf = grid_rf.best_estimator_

# test the best model
y_test_pred = best_rf.predict(X_test)
print("R2 score:", r2_score(y_test, y_test_pred))
print("Mean squared error:", mean_squared_error(y_test, y_test_pred))
print("Mean absolute error:", mean_absolute_error(y_test, y_test_pred))

# define the pipeline
dt_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('dt', DecisionTreeRegressor())
])

# define the hyperparameters to tune
params_dt = {
    'dt__max_depth': [5, 10, 15],
    'dt__min_samples_split': [2, 5, 10],
    'dt__min_samples_leaf': [1, 2, 4],
}

# define the grid search object
grid_dt = GridSearchCV(dt_pipe, params_dt, cv=5, n_jobs=-1, verbose=1)

# fit the grid search object to the data
grid_dt.fit(X_train, y_train)

# print the best parameters and score
print("Best parameters: ", grid_dt.best_params_)
print("Best score: ", grid_dt.best_score_)

# get the best model
best_dt = grid_dt.best_estimator_

# test the best model
y_test_pred = best_dt.predict(X_test)
print("R2 score:", r2_score(y_test, y_test_pred))
print("Mean squared error:", mean_squared_error(y_test, y_test_pred))
print("Mean absolute error:", mean_absolute_error(y_test, y_test_pred))

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor

# define the pipeline
bt_pipe = Pipeline([
    ('scale', StandardScaler()),
    ('xgb', XGBRegressor())
])

# define the hyperparameters to tune
params_bt = {
    'xgb__n_estimators': [50, 100, 200],
    'xgb__max_depth': [5, 10, 15],
    'xgb__learning_rate': [0.01, 0.1, 1],
}

# define the grid search object
grid_bt = GridSearchCV(bt_pipe, params_bt, cv=5, n_jobs=-1, verbose=1)

# fit the grid search object to the data
grid_bt.fit(X_train, y_train)

# print the best parameters and score
print("Best parameters: ", grid_bt.best_params_)
print("Best score: ", grid_bt.best_score_)

# get the best model
best_bt = grid_bt.best_estimator_

# test the best model
y_test_pred = best_bt.predict(X_test)
print("R2 score:", r2_score(y_test, y_test_pred))
print("Mean squared error:", mean_squared_error(y_test, y_test_pred))
print("Mean absolute error:", mean_absolute_error(y_test, y_test_pred))

y_test = y_test.reset_index(drop=True)

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np

rf_pipe.fit(X_train, y_train)
bt_pipe.fit(X_train, y_train)

# Predict the values of each model
rf_pred = rf_pipe.predict(X_test)
bt_pred = bt_pipe.predict(X_test)

# Sort the predicted values in descending order
rf_indices = np.argsort(rf_pred)[::-1]
bt_indices = np.argsort(bt_pred)[::-1]

# Calculate the cumulative gains for each model
rf_cumulative_gains = np.cumsum(y_test[rf_indices])
bt_cumulative_gains = np.cumsum(y_test[bt_indices])

# Calculate the total number of positive outcomes in the test set
total_positives = np.sum(y_test)

# Calculate the baseline cumulative gains
baseline_cumulative_gains = np.linspace(0, total_positives, len(y_test))

# Calculate the percentage of data
percentage_data = np.linspace(0, 100, len(y_test))

# Calculate the percentage of positive outcomes
rf_percentage_positives = 100 * rf_cumulative_gains / total_positives
bt_percentage_positives = 100 * bt_cumulative_gains / total_positives
baseline_percentage_positives = 100 * baseline_cumulative_gains / total_positives

# Plot the gains chart
plt.plot(percentage_data, rf_percentage_positives, label='Random Forest')
plt.plot(percentage_data, bt_percentage_positives, label='Boosted Tree')
plt.plot(percentage_data, baseline_percentage_positives, label='Baseline')
plt.xlabel('% of data')
plt.ylabel('% of positives')
plt.legend()
plt.show()